{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWlRDOHY1CIw"
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "![Logistic Regression](https://drive.google.com/uc?export=view&id=1_zRhqUmMdznHJNGWQuH050wvr-5n_2WS)\n",
    "\n",
    "**[Click Here](https://youtu.be/l8VEth6leXA) for the YouTube Video.**\n",
    "\n",
    "In statistics logistic regression is used to model the probability of a certain class or event. I will be focusing more on the basics and implementation of the model, and not go too deep into the math part in this post.\n",
    "\n",
    "Logistic regression is similar to linear regression because both of these involve estimating the values of parameters used in the prediction equation based on the given training data. Linear regression predicts the value of some continuous, dependent variable. Whereas logistic regression predicts the probability of an event or class that is dependent on other factors. Thus the output of logistic regression always lies between 0 and 1. Because of this property it is commonly used for classification purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkKuz_oeyuTI"
   },
   "source": [
    "\n",
    "## Logistic Model\n",
    "Consider a model with features *x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub> … x<sub>n</sub>*. Let the binary output be denoted by *Y*, that can take the values 0 or 1.  \n",
    "Let *p* be the probability of *Y = 1*, we can denote it as *p = P(Y=1)*.  \n",
    "The mathematical relationship between these variables can be denoted as:  \n",
    "\n",
    "$$ ln(\\frac{p}{1-p}) = b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n $$\n",
    "\n",
    "Here the term $\\frac{p}{1-p}$ is known as the *odds* and denotes the likelihood of the event taking place. Thus $ln(\\frac{p}{1-p})$ is known as the *log odds* and is simply used to map the probabilty that lies between 0 and 1 to a range between $(-\\infty, +\\infty)$. The terms $b_0, b_1, b_2 ...$ are parameters (or weights) that we will estimate during training.   \n",
    "\n",
    "So this is just the basic math behind what we are going to do. We are interested in the probability *p* in this equation. So we simplify the equation to obtain the value of p:  \n",
    "1. The log term $ln$ on the LHS can be removed by raising the RHS as a power of $e$:  \n",
    "$$ \\frac{p}{1-p}  = e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n} $$\n",
    "2. Now we can easily simplify to obtain the value of $p$:  \n",
    "$$ p = \\frac{e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n}}{1 + e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n}} $$\n",
    "$$ or $$\n",
    "$$ p = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n)}} $$   \n",
    "\n",
    "This actually turns out to be the equation of the *Sigmoid Function* which is widely used in other machine learning applications. The *Sigmoid Function* is given by:\n",
    "$$ S(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Now we will be using the above derived equation to make our predictions. Before that we will train our model to obtain the values of our parameters $b_0, b_1, b_2 ...$ that result in least error. This is where the error or loss function comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6n-a4xDy5u_"
   },
   "source": [
    "## Loss Function\n",
    "The loss is basically the error in our predicted value. In other words it is a difference between our predicted value and the actual value. We will be using the [L2 Loss Function](https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions) to calculate the error. Theoretically you can use any function to calculate the error. This function can be broken down as:\n",
    "1. Let the actual value be $y_i$. Let the value predicted using our model be denoted as $\\bar{y_i}$. Find the difference between the actual and predicted value. \n",
    "2. Square this difference. \n",
    "3. Find the sum across all the values in training data. \n",
    "$$ L = \\sum_{i = 1}^n(y_i - \\bar{y_i})^2 $$  \n",
    "\n",
    "Now that we have the error, we need to update the values of our parameters to minimize this error. This is where the \"learning\" actually happens, since our model is updating itself based on it's previous output to obtain a more accurate output in the next step. We will be using the *Gradient Descent Algorithm* to estimate our parameters. Another commonly used algorithm is the [Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_hesTCwy_gi"
   },
   "source": [
    "\n",
    "## The Gradient Descent Algorithm\n",
    "You might know that the partial derivative of a function at it's minimum value is equal to 0. So gradient descent basically uses this concept to estimate the parameters or weights of our model by minimizing the loss function. [Click here](https://www.youtube.com/watch?v=4PHI11lX11I) for a more detailed explanation on how gradient descent works.  \n",
    "For simplicity, for the rest of this tutorial let us assume that our ouptput depends only on a single feature $x$. So we can rewrite our equation as:\n",
    "$$ \\bar{y_i} = p = \\frac{1}{1 + e^{-(b_0 + b_1x_i)}} = \\frac{1}{1 + e^{-b_0 -b_1x_i}} $$  \n",
    "Thus we need to estimate the values of weights $b_0$ and $b_1$ using our given training data.\n",
    "1. Initially let $b_0=0$ and $b_1=0$. Let $L$ be the learning rate. The learning rate controls by how much the values of $b_0$ and $b_1$ are updated at each step in the learning process. Here let $L = 0.001$.\n",
    "2. Calculate the partial derivative with respect to $b_0$ and $b_1$. The value of the partial derivative will tell us how far the loss function is from it's minimum value. It is a measure of how much our weights need to be updated to attain minimum or ideally 0 error. In case you have more than one feature, you need to calculate the partial derivative for each weight $b_0$, $b_1$ ... $b_n$ where $n$ is the number of features. For a detailed explanation on the math behind calculating the partial derivatives, check out [my video.](https://youtu.be/l8VEth6leXA)\n",
    "$$ D_{b_0} = -2 \\sum_{i=1}^n(y_i - \\bar{y_i}) \\times \\bar{y_i} \\times (1 - \\bar{y_i}) $$\n",
    "$$ D_{b_1} = -2 \\sum_{i=1}^n(y_i - \\bar{y_i}) \\times \\bar{y_i} \\times (1 - \\bar{y_i}) \\times x_i $$\n",
    "3. Next we update the values of $b_0$ and $b_1$:\n",
    "$$ b_0 = b_0 - L \\times D_{b_0} $$\n",
    "$$ b_1 = b_1 - L \\times D_{b_1} $$  \n",
    "4. We repeat this process until our loss function is a very small value or ideally reaches 0 (meaning no errors and 100% accuracy). The number of times we repeat this learning process is known as iterations or epochs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aUmzO9SzFzo"
   },
   "source": [
    "## Implementing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "TNSeN-ZzybHN",
    "outputId": "3882d946-cd59-4e91-9b4d-887beb5e101b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6808361175ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Load the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import exp\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# Download the dataset\n",
    "# Source of dataset - https://www.kaggle.com/rakeshrau/social-network-ads\n",
    "!wget \"https://drive.google.com/uc?id=15WAD9_4CpUK6EWmgWVXU8YMnyYLKQvW8&export=download\" -O data.csv -q\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "b3R547WLzqEP",
    "outputId": "2ffe111e-a546-4cbc-c697-c7df896d209c"
   },
   "outputs": [],
   "source": [
    "# Visualizing the dataset\n",
    "plt.scatter(data['Age'], data['Purchased'])\n",
    "plt.show()\n",
    "\n",
    "# Divide the data to training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Age'], data['Purchased'], test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_x2wiimzzQg"
   },
   "outputs": [],
   "source": [
    "# Creating the logistic regression model\n",
    "\n",
    "# Helper function to normalize data\n",
    "def normalize(X):\n",
    "    return X - X.mean()\n",
    "\n",
    "# Method to make predictions\n",
    "def predict(X, b0, b1):\n",
    "    return np.array([1 / (1 + exp(-1*b0 + -1*b1*x)) for x in X])\n",
    "\n",
    "# Method to train the model\n",
    "def logistic_regression(X, Y):\n",
    "\n",
    "    X = normalize(X)\n",
    "\n",
    "    # Initializing variables\n",
    "    b0 = 0\n",
    "    b1 = 0\n",
    "    L = 0.001\n",
    "    epochs = 300\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = predict(X, b0, b1)\n",
    "        D_b0 = -2 * sum((Y - y_pred) * y_pred * (1 - y_pred))  # Derivative of loss wrt b0\n",
    "        D_b1 = -2 * sum(X * (Y - y_pred) * y_pred * (1 - y_pred))  # Derivative of loss wrt b1\n",
    "        b0 = b0 - L * D_b0\n",
    "        b1 = b1 - L * D_b1\n",
    "    \n",
    "    return b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "CibVQeLC0WyX",
    "outputId": "e5efd281-39c3-4730-b8dc-13464f220cb0"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "b0, b1 = logistic_regression(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "# X_test = X_test.sort_values()  # Sorting values is optional only to see the line graph\n",
    "X_test_norm = normalize(X_test)\n",
    "y_pred = predict(X_test_norm, b0, b1)\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.scatter(X_test, y_pred, c=\"red\")\n",
    "# plt.plot(X_test, y_pred, c=\"red\", linestyle='-', marker='o') # Only if values are sorted\n",
    "plt.show()\n",
    "\n",
    "# The accuracy\n",
    "accuracy = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test.iloc[i]:\n",
    "        accuracy += 1\n",
    "print(f\"Accuracy = {accuracy / len(y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "zdSdhARu1BLV",
    "outputId": "c28d6f05-3074-41b4-9ad1-8d7c7c43a1f7"
   },
   "outputs": [],
   "source": [
    "# Making predictions using scikit learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance and fit the model \n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train.values.reshape(-1, 1), y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Making predictions\n",
    "y_pred_sk = lr_model.predict(X_test.values.reshape(-1, 1))\n",
    "plt.clf()\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.scatter(X_test, y_pred_sk, c=\"red\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Accuracy = {lr_model.score(X_test.values.reshape(-1, 1), y_test.values.reshape(-1, 1))}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Logistic Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
